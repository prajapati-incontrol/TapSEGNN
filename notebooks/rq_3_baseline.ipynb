{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebbb1864",
   "metadata": {},
   "source": [
    "# Study - Comparison with Baseline methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92dc92f",
   "metadata": {},
   "source": [
    "<!-- All results are saved in `GD4PS/notebooks/rq3_results/[current_time]_results.md` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f66712c",
   "metadata": {},
   "source": [
    "### Methodology \n",
    "1. Initialise the dataset. \n",
    "2. Initialise the following models: FCNN, GCNN+GAT, GAT, Linegraph Laplacian and SEGNN. \n",
    "3. Performance for SE: Train these models on Net 42-A, evaluate the results. \n",
    "4. Performance for Generalisability: Train these models on Net 42-B, deploy on Net 42-A, evaluate the results. \n",
    "5. Performance for Scalability: Train these models on MVO, evaluate the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d8a5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x119156310>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch \n",
    "import os \n",
    "import sys \n",
    "\n",
    "# to access the models and utils \n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from src.dataset.custom_dataset import NodeEdgeTapDatasetV2 \n",
    "from src.training.trainer import trainer \n",
    "from utils.model_utils import initialize_model, get_eval_results \n",
    "from utils.gen_utils import dataset_splitter, get_device, load_config, plot_va_predictions_notebook\n",
    "from utils.ppnet_utils import initialize_network \n",
    "from utils.load_data_utils import load_sampled_input_data \n",
    "from src.model.graph_model import FCNNRegressor\n",
    "\n",
    "# for consistency among all the models \n",
    "torch.manual_seed(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f2b67",
   "metadata": {},
   "source": [
    "### Train and evaluate all five models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06b3be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Indices for MVO are available from [0,141] \n",
      "\n",
      "Number of Trafos = 143 \n",
      " \n",
      "Network: MVO is selected \n",
      "\n",
      "Net MVO has 320 nodes and 318 edges. \n",
      "\n",
      "Filling nan as 0 in tap_pos, tap_neutral, tap_step_degree\n",
      "Scaling inputs...\n",
      "Number of V, P measurements 301 out of 640\n",
      "\n",
      "Number of P_to, Q_to, P_from, Q_from measurements 1788 out of 1944\n",
      "\n",
      "Dataset for NEGATRegressor selected!\n",
      "\n",
      "\n",
      " Directed power flows accounted in dataset...\n",
      "\n",
      "\n",
      " get_edge_index_lu handling dictionary of tensors...\n",
      "\n",
      "At epoch: 0, \t training loss: 3.050e+00,                     \t validation loss: 2.106e+00 \t lr: 1.00e-02 \t grad_norm: 8.451e+00\n",
      "At epoch: 1, \t training loss: 1.989e+00,                     \t validation loss: 9.364e-01 \t lr: 1.00e-02 \t grad_norm: 6.001e+00\n",
      "At epoch: 2, \t training loss: 9.147e-01,                     \t validation loss: 9.756e-01 \t lr: 1.00e-02 \t grad_norm: 1.079e+00\n",
      "At epoch: 3, \t training loss: 9.525e-01,                     \t validation loss: 9.770e-01 \t lr: 1.00e-02 \t grad_norm: 4.956e-01\n",
      "At epoch: 4, \t training loss: 9.524e-01,                     \t validation loss: 9.652e-01 \t lr: 1.00e-02 \t grad_norm: 8.379e-02\n",
      "At epoch: 5, \t training loss: 9.401e-01,                     \t validation loss: 9.524e-01 \t lr: 1.00e-02 \t grad_norm: 8.242e-02\n",
      "At epoch: 6, \t training loss: 9.273e-01,                     \t validation loss: 9.376e-01 \t lr: 1.00e-02 \t grad_norm: 9.000e-02\n",
      "At epoch: 7, \t training loss: 9.132e-01,                     \t validation loss: 9.212e-01 \t lr: 1.00e-02 \t grad_norm: 1.588e-01\n",
      "At epoch: 8, \t training loss: 8.961e-01,                     \t validation loss: 8.915e-01 \t lr: 1.00e-02 \t grad_norm: 2.730e-01\n",
      "At epoch: 9, \t training loss: 8.672e-01,                     \t validation loss: 8.329e-01 \t lr: 1.00e-02 \t grad_norm: 3.708e-01\n",
      "At epoch: 10, \t training loss: 8.102e-01,                     \t validation loss: 7.202e-01 \t lr: 1.00e-02 \t grad_norm: 4.535e-01\n",
      "At epoch: 11, \t training loss: 7.000e-01,                     \t validation loss: 5.729e-01 \t lr: 1.00e-02 \t grad_norm: 5.029e-01\n",
      "At epoch: 12, \t training loss: 5.552e-01,                     \t validation loss: 4.487e-01 \t lr: 1.00e-02 \t grad_norm: 5.216e-01\n",
      "At epoch: 13, \t training loss: 4.317e-01,                     \t validation loss: 3.542e-01 \t lr: 1.00e-02 \t grad_norm: 6.545e-01\n",
      "At epoch: 14, \t training loss: 3.386e-01,                     \t validation loss: 2.549e-01 \t lr: 1.00e-02 \t grad_norm: 7.970e-01\n",
      "At epoch: 15, \t training loss: 2.432e-01,                     \t validation loss: 1.781e-01 \t lr: 1.00e-02 \t grad_norm: 6.547e-01\n",
      "At epoch: 16, \t training loss: 1.699e-01,                     \t validation loss: 1.556e-01 \t lr: 1.00e-02 \t grad_norm: 3.471e-01\n",
      "At epoch: 17, \t training loss: 1.478e-01,                     \t validation loss: 1.581e-01 \t lr: 1.00e-02 \t grad_norm: 3.412e-01\n",
      "At epoch: 18, \t training loss: 1.493e-01,                     \t validation loss: 1.502e-01 \t lr: 1.00e-02 \t grad_norm: 4.667e-01\n",
      "At epoch: 19, \t training loss: 1.415e-01,                     \t validation loss: 1.380e-01 \t lr: 1.00e-02 \t grad_norm: 3.659e-01\n",
      "At epoch: 20, \t training loss: 1.306e-01,                     \t validation loss: 1.316e-01 \t lr: 1.00e-02 \t grad_norm: 1.756e-01\n",
      "At epoch: 21, \t training loss: 1.241e-01,                     \t validation loss: 1.237e-01 \t lr: 1.00e-02 \t grad_norm: 2.904e-01\n",
      "At epoch: 22, \t training loss: 1.142e-01,                     \t validation loss: 1.137e-01 \t lr: 1.00e-02 \t grad_norm: 3.376e-01\n",
      "At epoch: 23, \t training loss: 1.022e-01,                     \t validation loss: 1.112e-01 \t lr: 1.00e-02 \t grad_norm: 1.369e-01\n",
      "At epoch: 24, \t training loss: 9.979e-02,                     \t validation loss: 1.055e-01 \t lr: 1.00e-02 \t grad_norm: 3.051e-01\n",
      "At epoch: 25, \t training loss: 9.521e-02,                     \t validation loss: 1.023e-01 \t lr: 1.00e-02 \t grad_norm: 2.395e-01\n",
      "At epoch: 26, \t training loss: 9.275e-02,                     \t validation loss: 1.030e-01 \t lr: 1.00e-02 \t grad_norm: 9.775e-02\n",
      "At epoch: 27, \t training loss: 9.407e-02,                     \t validation loss: 9.839e-02 \t lr: 1.00e-02 \t grad_norm: 2.552e-01\n",
      "At epoch: 28, \t training loss: 8.998e-02,                     \t validation loss: 9.573e-02 \t lr: 1.00e-02 \t grad_norm: 1.190e-01\n",
      "At epoch: 29, \t training loss: 8.763e-02,                     \t validation loss: 9.318e-02 \t lr: 1.00e-02 \t grad_norm: 1.882e-01\n",
      "Model training took 0.030 seconds.\n",
      "Test Loss of the resulting model is 1.00958884e-01\n",
      "At epoch: 0, \t training loss: 2.451e+00,                     \t validation loss: 1.399e+00 \t lr: 1.00e-02 \t grad_norm: 8.019e+00\n",
      "At epoch: 1, \t training loss: 1.383e+00,                     \t validation loss: 1.469e+00 \t lr: 1.00e-02 \t grad_norm: 2.977e+00\n",
      "At epoch: 2, \t training loss: 1.457e+00,                     \t validation loss: 1.200e+00 \t lr: 1.00e-02 \t grad_norm: 4.432e+00\n",
      "At epoch: 3, \t training loss: 1.158e+00,                     \t validation loss: 1.303e+00 \t lr: 1.00e-02 \t grad_norm: 2.019e+00\n",
      "At epoch: 4, \t training loss: 1.256e+00,                     \t validation loss: 1.171e+00 \t lr: 1.00e-02 \t grad_norm: 4.568e+00\n",
      "At epoch: 5, \t training loss: 1.136e+00,                     \t validation loss: 1.076e+00 \t lr: 1.00e-02 \t grad_norm: 3.143e+00\n",
      "At epoch: 6, \t training loss: 1.038e+00,                     \t validation loss: 1.069e+00 \t lr: 1.00e-02 \t grad_norm: 3.840e+00\n",
      "At epoch: 7, \t training loss: 1.013e+00,                     \t validation loss: 1.008e+00 \t lr: 1.00e-02 \t grad_norm: 4.715e+00\n",
      "At epoch: 8, \t training loss: 9.421e-01,                     \t validation loss: 8.911e-01 \t lr: 1.00e-02 \t grad_norm: 4.262e+00\n",
      "At epoch: 9, \t training loss: 8.287e-01,                     \t validation loss: 8.765e-01 \t lr: 1.00e-02 \t grad_norm: 2.943e+00\n",
      "At epoch: 10, \t training loss: 8.319e-01,                     \t validation loss: 8.655e-01 \t lr: 1.00e-02 \t grad_norm: 3.353e+00\n",
      "At epoch: 11, \t training loss: 8.292e-01,                     \t validation loss: 7.795e-01 \t lr: 1.00e-02 \t grad_norm: 3.795e+00\n",
      "At epoch: 12, \t training loss: 7.383e-01,                     \t validation loss: 7.434e-01 \t lr: 1.00e-02 \t grad_norm: 3.727e+00\n",
      "At epoch: 13, \t training loss: 6.929e-01,                     \t validation loss: 7.949e-01 \t lr: 1.00e-02 \t grad_norm: 2.337e+00\n",
      "At epoch: 14, \t training loss: 7.381e-01,                     \t validation loss: 7.397e-01 \t lr: 1.00e-02 \t grad_norm: 4.442e+00\n",
      "At epoch: 15, \t training loss: 6.832e-01,                     \t validation loss: 6.741e-01 \t lr: 1.00e-02 \t grad_norm: 3.734e+00\n",
      "At epoch: 16, \t training loss: 6.262e-01,                     \t validation loss: 6.838e-01 \t lr: 1.00e-02 \t grad_norm: 2.058e+00\n",
      "At epoch: 17, \t training loss: 6.421e-01,                     \t validation loss: 6.591e-01 \t lr: 1.00e-02 \t grad_norm: 3.116e+00\n",
      "At epoch: 18, \t training loss: 6.193e-01,                     \t validation loss: 6.131e-01 \t lr: 1.00e-02 \t grad_norm: 3.301e+00\n",
      "At epoch: 19, \t training loss: 5.737e-01,                     \t validation loss: 6.134e-01 \t lr: 1.00e-02 \t grad_norm: 1.524e+00\n",
      "At epoch: 20, \t training loss: 5.755e-01,                     \t validation loss: 6.040e-01 \t lr: 1.00e-02 \t grad_norm: 2.272e+00\n",
      "At epoch: 21, \t training loss: 5.661e-01,                     \t validation loss: 5.670e-01 \t lr: 1.00e-02 \t grad_norm: 2.660e+00\n",
      "At epoch: 22, \t training loss: 5.305e-01,                     \t validation loss: 5.544e-01 \t lr: 1.00e-02 \t grad_norm: 1.643e+00\n",
      "At epoch: 23, \t training loss: 5.233e-01,                     \t validation loss: 5.613e-01 \t lr: 1.00e-02 \t grad_norm: 1.059e+00\n",
      "At epoch: 24, \t training loss: 5.335e-01,                     \t validation loss: 5.366e-01 \t lr: 1.00e-02 \t grad_norm: 2.047e+00\n",
      "At epoch: 25, \t training loss: 5.095e-01,                     \t validation loss: 5.133e-01 \t lr: 1.00e-02 \t grad_norm: 1.691e+00\n",
      "At epoch: 26, \t training loss: 4.845e-01,                     \t validation loss: 5.224e-01 \t lr: 1.00e-02 \t grad_norm: 5.145e-01\n",
      "At epoch: 27, \t training loss: 4.919e-01,                     \t validation loss: 5.235e-01 \t lr: 1.00e-02 \t grad_norm: 1.464e+00\n",
      "At epoch: 28, \t training loss: 4.913e-01,                     \t validation loss: 5.044e-01 \t lr: 1.00e-02 \t grad_norm: 1.693e+00\n",
      "At epoch: 29, \t training loss: 4.738e-01,                     \t validation loss: 4.918e-01 \t lr: 1.00e-02 \t grad_norm: 1.042e+00\n",
      "Model training took 0.742 seconds.\n",
      "Test Loss of the resulting model is 4.53970402e-01\n",
      "At epoch: 0, \t training loss: 2.008e+00,                     \t validation loss: 1.899e+00 \t lr: 1.00e-02 \t grad_norm: 2.475e+00\n",
      "At epoch: 1, \t training loss: 1.830e+00,                     \t validation loss: 1.728e+00 \t lr: 1.00e-02 \t grad_norm: 2.233e+00\n",
      "At epoch: 2, \t training loss: 1.668e+00,                     \t validation loss: 1.566e+00 \t lr: 1.00e-02 \t grad_norm: 2.021e+00\n",
      "At epoch: 3, \t training loss: 1.516e+00,                     \t validation loss: 1.410e+00 \t lr: 1.00e-02 \t grad_norm: 1.847e+00\n",
      "At epoch: 4, \t training loss: 1.368e+00,                     \t validation loss: 1.260e+00 \t lr: 1.00e-02 \t grad_norm: 1.707e+00\n",
      "At epoch: 5, \t training loss: 1.228e+00,                     \t validation loss: 1.128e+00 \t lr: 1.00e-02 \t grad_norm: 1.587e+00\n",
      "At epoch: 6, \t training loss: 1.105e+00,                     \t validation loss: 1.024e+00 \t lr: 1.00e-02 \t grad_norm: 1.377e+00\n",
      "At epoch: 7, \t training loss: 1.009e+00,                     \t validation loss: 9.475e-01 \t lr: 1.00e-02 \t grad_norm: 1.136e+00\n",
      "At epoch: 8, \t training loss: 9.365e-01,                     \t validation loss: 8.946e-01 \t lr: 1.00e-02 \t grad_norm: 9.251e-01\n",
      "At epoch: 9, \t training loss: 8.846e-01,                     \t validation loss: 8.636e-01 \t lr: 1.00e-02 \t grad_norm: 7.217e-01\n",
      "At epoch: 10, \t training loss: 8.520e-01,                     \t validation loss: 8.514e-01 \t lr: 1.00e-02 \t grad_norm: 5.419e-01\n",
      "At epoch: 11, \t training loss: 8.368e-01,                     \t validation loss: 8.540e-01 \t lr: 1.00e-02 \t grad_norm: 4.503e-01\n",
      "At epoch: 12, \t training loss: 8.353e-01,                     \t validation loss: 8.655e-01 \t lr: 1.00e-02 \t grad_norm: 4.917e-01\n",
      "At epoch: 13, \t training loss: 8.425e-01,                     \t validation loss: 8.792e-01 \t lr: 1.00e-02 \t grad_norm: 6.350e-01\n",
      "At epoch: 14, \t training loss: 8.530e-01,                     \t validation loss: 8.896e-01 \t lr: 1.00e-02 \t grad_norm: 7.878e-01\n",
      "At epoch: 15, \t training loss: 8.616e-01,                     \t validation loss: 8.930e-01 \t lr: 1.00e-02 \t grad_norm: 9.083e-01\n",
      "At epoch: 16, \t training loss: 8.648e-01,                     \t validation loss: 8.882e-01 \t lr: 1.00e-02 \t grad_norm: 9.831e-01\n",
      "At epoch: 17, \t training loss: 8.616e-01,                     \t validation loss: 8.767e-01 \t lr: 1.00e-02 \t grad_norm: 1.005e+00\n",
      "At epoch: 18, \t training loss: 8.529e-01,                     \t validation loss: 8.614e-01 \t lr: 1.00e-02 \t grad_norm: 9.747e-01\n",
      "At epoch: 19, \t training loss: 8.409e-01,                     \t validation loss: 8.473e-01 \t lr: 1.00e-02 \t grad_norm: 9.109e-01\n",
      "At epoch: 20, \t training loss: 8.298e-01,                     \t validation loss: 8.344e-01 \t lr: 1.00e-02 \t grad_norm: 8.409e-01\n",
      "At epoch: 21, \t training loss: 8.197e-01,                     \t validation loss: 8.219e-01 \t lr: 1.00e-02 \t grad_norm: 7.624e-01\n",
      "At epoch: 22, \t training loss: 8.093e-01,                     \t validation loss: 8.098e-01 \t lr: 1.00e-02 \t grad_norm: 6.862e-01\n",
      "At epoch: 23, \t training loss: 7.989e-01,                     \t validation loss: 7.987e-01 \t lr: 1.00e-02 \t grad_norm: 6.206e-01\n",
      "At epoch: 24, \t training loss: 7.886e-01,                     \t validation loss: 7.903e-01 \t lr: 1.00e-02 \t grad_norm: 5.343e-01\n",
      "At epoch: 25, \t training loss: 7.807e-01,                     \t validation loss: 7.855e-01 \t lr: 1.00e-02 \t grad_norm: 3.979e-01\n",
      "At epoch: 26, \t training loss: 7.755e-01,                     \t validation loss: 7.826e-01 \t lr: 1.00e-02 \t grad_norm: 3.225e-01\n",
      "At epoch: 27, \t training loss: 7.719e-01,                     \t validation loss: 7.817e-01 \t lr: 1.00e-02 \t grad_norm: 2.625e-01\n",
      "At epoch: 28, \t training loss: 7.700e-01,                     \t validation loss: 7.825e-01 \t lr: 1.00e-02 \t grad_norm: 2.298e-01\n",
      "At epoch: 29, \t training loss: 7.697e-01,                     \t validation loss: 7.844e-01 \t lr: 1.00e-02 \t grad_norm: 2.414e-01\n",
      "Model training took 0.501 seconds.\n",
      "Test Loss of the resulting model is 7.71574020e-01\n",
      "At epoch: 0, \t training loss: 5.505e+00,                     \t validation loss: 5.344e+00 \t lr: 1.00e-02 \t grad_norm: 2.264e+01\n",
      "At epoch: 1, \t training loss: 5.585e+00,                     \t validation loss: 5.740e+00 \t lr: 1.00e-02 \t grad_norm: 5.536e+01\n",
      "At epoch: 2, \t training loss: 5.699e+00,                     \t validation loss: 3.220e+00 \t lr: 1.00e-02 \t grad_norm: 4.883e+01\n",
      "At epoch: 3, \t training loss: 3.139e+00,                     \t validation loss: 2.126e+00 \t lr: 1.00e-02 \t grad_norm: 2.379e+01\n",
      "At epoch: 4, \t training loss: 2.106e+00,                     \t validation loss: 2.251e+00 \t lr: 1.00e-02 \t grad_norm: 1.586e+01\n",
      "At epoch: 5, \t training loss: 2.283e+00,                     \t validation loss: 2.022e+00 \t lr: 1.00e-02 \t grad_norm: 2.148e+01\n",
      "At epoch: 6, \t training loss: 2.057e+00,                     \t validation loss: 1.990e+00 \t lr: 1.00e-02 \t grad_norm: 1.525e+01\n",
      "At epoch: 7, \t training loss: 2.010e+00,                     \t validation loss: 2.108e+00 \t lr: 1.00e-02 \t grad_norm: 9.254e+00\n",
      "At epoch: 8, \t training loss: 2.095e+00,                     \t validation loss: 2.004e+00 \t lr: 1.00e-02 \t grad_norm: 1.066e+01\n",
      "At epoch: 9, \t training loss: 1.963e+00,                     \t validation loss: 1.629e+00 \t lr: 1.00e-02 \t grad_norm: 1.138e+01\n",
      "At epoch: 10, \t training loss: 1.576e+00,                     \t validation loss: 1.253e+00 \t lr: 1.00e-02 \t grad_norm: 9.233e+00\n",
      "At epoch: 11, \t training loss: 1.199e+00,                     \t validation loss: 1.100e+00 \t lr: 1.00e-02 \t grad_norm: 5.098e+00\n",
      "At epoch: 12, \t training loss: 1.054e+00,                     \t validation loss: 1.178e+00 \t lr: 1.00e-02 \t grad_norm: 2.192e+00\n",
      "At epoch: 13, \t training loss: 1.143e+00,                     \t validation loss: 1.324e+00 \t lr: 1.00e-02 \t grad_norm: 4.663e+00\n",
      "At epoch: 14, \t training loss: 1.298e+00,                     \t validation loss: 1.351e+00 \t lr: 1.00e-02 \t grad_norm: 6.857e+00\n",
      "At epoch: 15, \t training loss: 1.328e+00,                     \t validation loss: 1.242e+00 \t lr: 1.00e-02 \t grad_norm: 7.192e+00\n",
      "At epoch: 16, \t training loss: 1.213e+00,                     \t validation loss: 1.091e+00 \t lr: 1.00e-02 \t grad_norm: 6.199e+00\n",
      "At epoch: 17, \t training loss: 1.054e+00,                     \t validation loss: 1.055e+00 \t lr: 1.00e-02 \t grad_norm: 3.532e+00\n",
      "At epoch: 18, \t training loss: 1.011e+00,                     \t validation loss: 1.070e+00 \t lr: 1.00e-02 \t grad_norm: 3.240e+00\n",
      "At epoch: 19, \t training loss: 1.021e+00,                     \t validation loss: 1.067e+00 \t lr: 1.00e-02 \t grad_norm: 3.958e+00\n",
      "At epoch: 20, \t training loss: 1.013e+00,                     \t validation loss: 1.045e+00 \t lr: 1.00e-02 \t grad_norm: 3.752e+00\n",
      "At epoch: 21, \t training loss: 9.874e-01,                     \t validation loss: 1.025e+00 \t lr: 1.00e-02 \t grad_norm: 2.971e+00\n",
      "At epoch: 22, \t training loss: 9.686e-01,                     \t validation loss: 1.014e+00 \t lr: 1.00e-02 \t grad_norm: 2.319e+00\n",
      "At epoch: 23, \t training loss: 9.623e-01,                     \t validation loss: 1.010e+00 \t lr: 1.00e-02 \t grad_norm: 2.560e+00\n",
      "At epoch: 24, \t training loss: 9.618e-01,                     \t validation loss: 9.956e-01 \t lr: 1.00e-02 \t grad_norm: 3.567e+00\n",
      "At epoch: 25, \t training loss: 9.486e-01,                     \t validation loss: 9.523e-01 \t lr: 1.00e-02 \t grad_norm: 3.932e+00\n",
      "At epoch: 26, \t training loss: 9.027e-01,                     \t validation loss: 9.199e-01 \t lr: 1.00e-02 \t grad_norm: 3.036e+00\n",
      "At epoch: 27, \t training loss: 8.683e-01,                     \t validation loss: 9.207e-01 \t lr: 1.00e-02 \t grad_norm: 1.767e+00\n",
      "At epoch: 28, \t training loss: 8.685e-01,                     \t validation loss: 9.208e-01 \t lr: 1.00e-02 \t grad_norm: 2.207e+00\n",
      "At epoch: 29, \t training loss: 8.686e-01,                     \t validation loss: 9.058e-01 \t lr: 1.00e-02 \t grad_norm: 2.474e+00\n",
      "Model training took 1.116 seconds.\n",
      "Test Loss of the resulting model is 8.39898646e-01\n",
      "At epoch: 0, \t training loss: 6.645e+00,                     \t validation loss: 1.293e+01 \t lr: 1.00e-02 \t grad_norm: 4.145e+01\n",
      "At epoch: 1, \t training loss: 1.279e+01,                     \t validation loss: 3.149e+00 \t lr: 1.00e-02 \t grad_norm: 9.626e+01\n",
      "At epoch: 2, \t training loss: 3.193e+00,                     \t validation loss: 4.601e+00 \t lr: 1.00e-02 \t grad_norm: 2.486e+01\n",
      "At epoch: 3, \t training loss: 4.717e+00,                     \t validation loss: 4.979e+00 \t lr: 1.00e-02 \t grad_norm: 4.176e+01\n",
      "At epoch: 4, \t training loss: 5.101e+00,                     \t validation loss: 2.919e+00 \t lr: 1.00e-02 \t grad_norm: 4.660e+01\n",
      "At epoch: 5, \t training loss: 2.992e+00,                     \t validation loss: 1.685e+00 \t lr: 1.00e-02 \t grad_norm: 2.877e+01\n",
      "At epoch: 6, \t training loss: 1.684e+00,                     \t validation loss: 1.657e+00 \t lr: 1.00e-02 \t grad_norm: 1.168e+01\n",
      "At epoch: 7, \t training loss: 1.578e+00,                     \t validation loss: 2.143e+00 \t lr: 1.00e-02 \t grad_norm: 9.284e+00\n",
      "At epoch: 8, \t training loss: 2.014e+00,                     \t validation loss: 2.442e+00 \t lr: 1.00e-02 \t grad_norm: 1.514e+01\n",
      "At epoch: 9, \t training loss: 2.300e+00,                     \t validation loss: 2.362e+00 \t lr: 1.00e-02 \t grad_norm: 1.649e+01\n",
      "At epoch: 10, \t training loss: 2.239e+00,                     \t validation loss: 1.995e+00 \t lr: 1.00e-02 \t grad_norm: 1.522e+01\n",
      "At epoch: 11, \t training loss: 1.906e+00,                     \t validation loss: 1.583e+00 \t lr: 1.00e-02 \t grad_norm: 1.209e+01\n",
      "At epoch: 12, \t training loss: 1.528e+00,                     \t validation loss: 1.279e+00 \t lr: 1.00e-02 \t grad_norm: 8.437e+00\n",
      "At epoch: 13, \t training loss: 1.248e+00,                     \t validation loss: 1.155e+00 \t lr: 1.00e-02 \t grad_norm: 5.225e+00\n",
      "At epoch: 14, \t training loss: 1.140e+00,                     \t validation loss: 1.269e+00 \t lr: 1.00e-02 \t grad_norm: 4.047e+00\n",
      "At epoch: 15, \t training loss: 1.266e+00,                     \t validation loss: 1.390e+00 \t lr: 1.00e-02 \t grad_norm: 7.864e+00\n",
      "At epoch: 16, \t training loss: 1.389e+00,                     \t validation loss: 1.362e+00 \t lr: 1.00e-02 \t grad_norm: 8.906e+00\n",
      "At epoch: 17, \t training loss: 1.357e+00,                     \t validation loss: 1.251e+00 \t lr: 1.00e-02 \t grad_norm: 8.142e+00\n",
      "At epoch: 18, \t training loss: 1.241e+00,                     \t validation loss: 1.198e+00 \t lr: 1.00e-02 \t grad_norm: 6.104e+00\n",
      "At epoch: 19, \t training loss: 1.178e+00,                     \t validation loss: 1.152e+00 \t lr: 1.00e-02 \t grad_norm: 7.205e+00\n",
      "At epoch: 20, \t training loss: 1.118e+00,                     \t validation loss: 1.109e+00 \t lr: 1.00e-02 \t grad_norm: 6.169e+00\n",
      "At epoch: 21, \t training loss: 1.066e+00,                     \t validation loss: 1.094e+00 \t lr: 1.00e-02 \t grad_norm: 3.873e+00\n",
      "At epoch: 22, \t training loss: 1.047e+00,                     \t validation loss: 1.081e+00 \t lr: 1.00e-02 \t grad_norm: 3.644e+00\n",
      "At epoch: 23, \t training loss: 1.034e+00,                     \t validation loss: 1.066e+00 \t lr: 1.00e-02 \t grad_norm: 3.450e+00\n",
      "At epoch: 24, \t training loss: 1.022e+00,                     \t validation loss: 1.052e+00 \t lr: 1.00e-02 \t grad_norm: 3.217e+00\n",
      "At epoch: 25, \t training loss: 1.011e+00,                     \t validation loss: 1.062e+00 \t lr: 1.00e-02 \t grad_norm: 2.889e+00\n",
      "At epoch: 26, \t training loss: 1.023e+00,                     \t validation loss: 1.068e+00 \t lr: 1.00e-02 \t grad_norm: 3.743e+00\n",
      "At epoch: 27, \t training loss: 1.029e+00,                     \t validation loss: 1.037e+00 \t lr: 1.00e-02 \t grad_norm: 4.063e+00\n",
      "At epoch: 28, \t training loss: 9.980e-01,                     \t validation loss: 1.011e+00 \t lr: 1.00e-02 \t grad_norm: 3.745e+00\n",
      "At epoch: 29, \t training loss: 9.715e-01,                     \t validation loss: 9.874e-01 \t lr: 1.00e-02 \t grad_norm: 3.318e+00\n",
      "Model training took 1.106 seconds.\n",
      "Test Loss of the resulting model is 9.20295596e-01\n"
     ]
    }
   ],
   "source": [
    "config['data']['load_std'] = 0.3\n",
    "net_MVO = initialize_network(net_name='MVO', else_load=config['data']['load_std'], verbose=True)\n",
    "\n",
    "# config['data']['num_samples'] = 80\n",
    "\n",
    "sampled_input_data_MVO = load_sampled_input_data(sc_type=config['data']['scenario_type'], \n",
    "                                                        net=net_MVO, \n",
    "                                                        num_samples=config['data']['num_samples'],\n",
    "                                                        noise=config['data']['noise'],\n",
    "                                                        trafo_ids=[], # foolproof\n",
    "                                                        scaler=config['data']['scaler'],\n",
    "                                                        )\n",
    "\n",
    "dataset_MVO = NodeEdgeTapDatasetV2(model_name=config['model']['name'], sampled_input_data=sampled_input_data_MVO)\n",
    "\n",
    "all_loaders_MVO, plot_loader_MVO = dataset_splitter(dataset_MVO,\n",
    "                                    batch_size=config['loader']['batch_size'], \n",
    "                                    split_list=config['loader']['split_list'])\n",
    "\n",
    "fcnn_dataset_MVO = FCNNDataset(sampled_input_data=sampled_input_data_MVO)\n",
    "\n",
    "fcnn_all_loaders_MVO, fcnn_plot_loader_MVO = dataset_splitter_fcnn(fcnn_dataset_MVO,\n",
    "                                            batch_size=config['loader']['batch_size'],\n",
    "                                            split_list=config['loader']['split_list'])\n",
    "\n",
    "num_nodes = len(net_MVO.bus.index)\n",
    "\n",
    "#######################################################################################################\n",
    "fcnn_model_MVO = FCNNRegressor(in_feat = num_node_features * num_nodes, \n",
    "                           hid_feat_list=[128], \n",
    "                           out_feat=num_y_node_features * num_nodes)\n",
    "\n",
    "optimizer_fcnn_MVO = torch.optim.Adam(fcnn_model_MVO.parameters(), \n",
    "                                  lr=config['training']['lr'],\n",
    "                                  weight_decay=0.01,   \n",
    "                                )\n",
    "schedular_fcnn_MVO = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_fcnn_MVO,\n",
    "                                                            mode='min',\n",
    "                                                            factor=0.1, \n",
    "                                                            min_lr=1e-4,\n",
    "                                                          )\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "model_NGAT_MVO = initialize_model(model_name=\"NGATRegressor\",\n",
    "                            dataset=dataset_MVO,\n",
    "                            node_out_features=config['model']['node_out_features'],\n",
    "                            list_node_hidden_features=config['model']['list_node_hidden_features'],\n",
    "                            k_hop_node=config['model']['k_hop_node'],\n",
    "                            edge_out_features=config['model']['edge_out_features'], \n",
    "                            list_edge_hidden_features=config['model']['list_edge_hidden_features'],\n",
    "                            k_hop_edge=config['model']['k_hop_edge'],\n",
    "                            trafo_hop=config['model']['trafo_hop'],\n",
    "                            edge_index_list=sampled_input_data['edge_index'],\n",
    "                            gat_out_features=config['model']['gat_out_features'],\n",
    "                            gat_head=config['model']['gat_head'],\n",
    "                            bias=config['model']['bias'], \n",
    "                            normalize=config['model']['normalize'], \n",
    "                            device=device,  \n",
    "                            ).to(device)    \n",
    "\n",
    "optimizer_NGAT_MVO = torch.optim.Adam(model_NGAT_MVO.parameters(),lr=config['training']['lr'], weight_decay=config['training']['weight_decay'])\n",
    "\n",
    "schedular_NGAT_MVO = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_NGAT_MVO, \n",
    "                                                    mode='min',\n",
    "                                                    factor=0.1, \n",
    "                                                    # patience=1,\n",
    "                                                    min_lr=config['training']['schedular_min_lr'])\n",
    "\n",
    "#######################################################################################################\n",
    "model_GAT_MVO = initialize_model(model_name=\"GATRegressor\",\n",
    "                            dataset=dataset_MVO,\n",
    "                            node_out_features=config['model']['node_out_features'],\n",
    "                            list_node_hidden_features=config['model']['list_node_hidden_features'],\n",
    "                            k_hop_node=config['model']['k_hop_node'],\n",
    "                            edge_out_features=config['model']['edge_out_features'], \n",
    "                            list_edge_hidden_features=config['model']['list_edge_hidden_features'],\n",
    "                            k_hop_edge=config['model']['k_hop_edge'],\n",
    "                            trafo_hop=config['model']['trafo_hop'],\n",
    "                            edge_index_list=sampled_input_data['edge_index'],\n",
    "                            gat_out_features=config['model']['gat_out_features'],\n",
    "                            gat_head=config['model']['gat_head'],\n",
    "                            bias=config['model']['bias'], \n",
    "                            normalize=config['model']['normalize'], \n",
    "                            device=device,  \n",
    "                            ).to(device)    \n",
    "\n",
    "optimizer_GAT_MVO = torch.optim.Adam(model_GAT_MVO.parameters(),lr=config['training']['lr'], weight_decay=config['training']['weight_decay'])\n",
    "\n",
    "schedular_GAT_MVO = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_GAT_MVO, \n",
    "                                                    mode='min',\n",
    "                                                    factor=0.1, \n",
    "                                                    # patience=1,\n",
    "                                                    min_lr=config['training']['schedular_min_lr'])\n",
    "\n",
    "#######################################################################################################\n",
    "model_LGL_MVO = initialize_model(model_name=\"NEGATRegressor_LGL\",\n",
    "                            dataset=dataset_MVO,\n",
    "                            node_out_features=config['model']['node_out_features'],\n",
    "                            list_node_hidden_features=config['model']['list_node_hidden_features'],\n",
    "                            k_hop_node=config['model']['k_hop_node'],\n",
    "                            edge_out_features=config['model']['edge_out_features'], \n",
    "                            list_edge_hidden_features=config['model']['list_edge_hidden_features'],\n",
    "                            k_hop_edge=config['model']['k_hop_edge'],\n",
    "                            trafo_hop=config['model']['trafo_hop'],\n",
    "                            edge_index_list=sampled_input_data['edge_index'],\n",
    "                            gat_out_features=config['model']['gat_out_features'],\n",
    "                            gat_head=config['model']['gat_head'],\n",
    "                            bias=config['model']['bias'], \n",
    "                            normalize=config['model']['normalize'], \n",
    "                            device=device,  \n",
    "                            ).to(device)\n",
    "\n",
    "optimizer_LGL_MVO = torch.optim.Adam(model_LGL_MVO.parameters(),lr=config['training']['lr'], weight_decay=config['training']['weight_decay'])\n",
    "\n",
    "schedular_LGL_MVO = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_LGL_MVO, \n",
    "                                                    mode='min',\n",
    "                                                    factor=0.1,\n",
    "                                                    min_lr=config['training']['schedular_min_lr'])\n",
    "\n",
    "#######################################################################################################\n",
    "model_MVO = initialize_model(model_name=config['model']['name'],\n",
    "                            dataset=dataset_MVO,\n",
    "                            node_out_features=config['model']['node_out_features'],\n",
    "                            list_node_hidden_features=config['model']['list_node_hidden_features'],\n",
    "                            k_hop_node=config['model']['k_hop_node'],\n",
    "                            edge_out_features=config['model']['edge_out_features'], \n",
    "                            list_edge_hidden_features=config['model']['list_edge_hidden_features'],\n",
    "                            k_hop_edge=config['model']['k_hop_edge'],\n",
    "                            trafo_hop=config['model']['trafo_hop'],\n",
    "                            edge_index_list=sampled_input_data['edge_index'],\n",
    "                            gat_out_features=config['model']['gat_out_features'],\n",
    "                            gat_head=config['model']['gat_head'],\n",
    "                            bias=config['model']['bias'], \n",
    "                            normalize=config['model']['normalize'], \n",
    "                            device=device,\n",
    "                            ).to(device)\n",
    "                            \n",
    "optimizer_MVO = torch.optim.Adam(model_MVO.parameters(),lr=config['training']['lr'], weight_decay=config['training']['weight_decay'])\n",
    "\n",
    "schedular_MVO = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_MVO, \n",
    "                                                    mode='min',\n",
    "                                                    factor=0.1, \n",
    "                                                    # patience=1,\n",
    "                                                    min_lr=config['training']['schedular_min_lr'])\n",
    "\n",
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "all_losses_FCNN_MVO = trainer(model=fcnn_model_MVO, \n",
    "                            train_loader=fcnn_all_loaders_MVO[0],\n",
    "                            val_loader=fcnn_all_loaders_MVO[1],\n",
    "                            test_loader=fcnn_all_loaders_MVO[2],\n",
    "                            optimizer=optimizer_fcnn_MVO, \n",
    "                            schedular=schedular_fcnn_MVO, \n",
    "                            num_epoch=config['training']['num_epochs'],\n",
    "                            early_stopping=config['training']['early_stopping'],\n",
    "                            val_patience=config['training']['val_patience'], \n",
    "                            device=device,\n",
    "                            )\n",
    "\n",
    "all_losses_NGAT_MVO = trainer(model=model_NGAT_MVO, \n",
    "                    train_loader=all_loaders_MVO[0], \n",
    "                    val_loader=all_loaders_MVO[1], \n",
    "                    test_loader=all_loaders_MVO[2], \n",
    "                    optimizer=optimizer_NGAT_MVO,\n",
    "                    schedular=schedular_NGAT_MVO,\n",
    "                    num_epoch=config['training']['num_epochs'],\n",
    "                    early_stopping=config['training']['early_stopping'],\n",
    "                    val_patience=config['training']['val_patience'],  \n",
    "                    device=device)\n",
    "\n",
    "all_losses_GAT_MVO = trainer(model=model_GAT_MVO, \n",
    "                    train_loader=all_loaders_MVO[0], \n",
    "                    val_loader=all_loaders_MVO[1], \n",
    "                    test_loader=all_loaders_MVO[2], \n",
    "                    optimizer=optimizer_GAT_MVO,\n",
    "                    schedular=schedular_GAT_MVO,\n",
    "                    num_epoch=config['training']['num_epochs'],\n",
    "                    early_stopping=config['training']['early_stopping'],\n",
    "                    val_patience=config['training']['val_patience'],  \n",
    "                    device=device)\n",
    "\n",
    "all_losses_MVO = trainer(model=model_LGL_MVO, \n",
    "                    train_loader=all_loaders_MVO[0], \n",
    "                    val_loader=all_loaders_MVO[1], \n",
    "                    test_loader=all_loaders_MVO[2], \n",
    "                    optimizer=optimizer_LGL_MVO,\n",
    "                    schedular=schedular_LGL_MVO,\n",
    "                    num_epoch=config['training']['num_epochs'],\n",
    "                    early_stopping=config['training']['early_stopping'],\n",
    "                    val_patience=config['training']['val_patience'],  \n",
    "                    device=device)\n",
    "\n",
    "all_losses_MVO = trainer(model=model_MVO, \n",
    "                    train_loader=all_loaders_MVO[0], \n",
    "                    val_loader=all_loaders_MVO[1], \n",
    "                    test_loader=all_loaders_MVO[2], \n",
    "                    optimizer=optimizer_MVO,\n",
    "                    schedular=schedular_MVO,\n",
    "                    num_epoch=config['training']['num_epochs'],\n",
    "                    early_stopping=config['training']['early_stopping'],\n",
    "                    val_patience=config['training']['val_patience'],  \n",
    "                    device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ac8c2",
   "metadata": {},
   "source": [
    "### Scalability results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5de3a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating results for StandardScaled Voltage and Angles.\n",
      "Calculating results for StandardScaled Voltage and Angles.\n",
      "Calculating results for StandardScaled Voltage and Angles.\n",
      "Calculating results for StandardScaled Voltage and Angles.\n"
     ]
    }
   ],
   "source": [
    "####################### FCNN Model #####################################\n",
    "results_fcnn_MVO = get_eval_results(test_loader=fcnn_all_loaders_MVO[2],\n",
    "                            tap_weight=config['training']['loss_tap_weight'], \n",
    "                            trained_model=fcnn_model_MVO, \n",
    "                            scaler=sampled_input_data['scaler_y_label'], \n",
    "                            fcnn=True, \n",
    "                            num_nodes=320)\n",
    "\n",
    "####################### GCNN+GAT #####################################\n",
    "results_ngat_MVO = get_eval_results(test_loader=all_loaders_MVO[2],\n",
    "                            tap_weight=config['training']['loss_tap_weight'], \n",
    "                            trained_model=model_NGAT_MVO, \n",
    "                            scaler=sampled_input_data['scaler_y_label'])\n",
    "\n",
    "####################### GAT #####################################\n",
    "results_gat_MVO = get_eval_results(test_loader=all_loaders_MVO[2],\n",
    "                            tap_weight=config['training']['loss_tap_weight'], \n",
    "                            trained_model=model_GAT_MVO, \n",
    "                            scaler=sampled_input_data['scaler_y_label'])\n",
    "\n",
    "####################### NEGAT_LG #####################################\n",
    "results_lg_MVO = get_eval_results(test_loader=all_loaders_MVO[2],\n",
    "                            tap_weight=config['training']['loss_tap_weight'], \n",
    "                            trained_model=model_LGL_MVO, \n",
    "                            scaler=sampled_input_data['scaler_y_label'])\n",
    "\n",
    "####################### NEGAT Regressor Model #####################################\n",
    "results_MVO = get_eval_results(test_loader=all_loaders_MVO[2],\n",
    "                            tap_weight=config['training']['loss_tap_weight'], \n",
    "                            trained_model=model_MVO, \n",
    "                            scaler=sampled_input_data['scaler_y_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8d776d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FCNN MVO Results:\n",
      "==================================================\n",
      "Batchwise Average Test Loss: 1.009589e-01\n",
      "\n",
      "RMSE_V                   : 8.194978e-04\n",
      "\n",
      "RMSE_A                   : 1.202922e+00\n",
      "\n",
      "MAE_V                    : 5.505631e-04\n",
      "\n",
      "MAE_A                    : 9.452806e-01\n",
      "\n",
      "MaxAE_V                  : 5.174160e-03\n",
      "\n",
      "MaxAE_A                  : 4.803596e+00\n",
      "\n",
      "NRMSE_V                  : 7.462014e-05\n",
      "\n",
      "NRMSE_A                  : -2.231518e-02\n",
      "\n",
      "\n",
      "==================================================\n",
      "GCNN+GAT MVO Results:\n",
      "==================================================\n",
      "Batchwise Average Test Loss: 4.539704e-01\n",
      "\n",
      "RMSE_V                   : 4.331977e-04\n",
      "\n",
      "RMSE_A                   : 8.093555e+00\n",
      "\n",
      "MAE_V                    : 3.318278e-04\n",
      "\n",
      "MAE_A                    : 5.154775e+00\n",
      "\n",
      "MaxAE_V                  : 3.573537e-03\n",
      "\n",
      "MaxAE_A                  : 2.960396e+01\n",
      "\n",
      "NRMSE_V                  : 3.943725e-05\n",
      "\n",
      "NRMSE_A                  : -1.511558e-01\n",
      "\n",
      "\n",
      "==================================================\n",
      "GAT MVO Results:\n",
      "==================================================\n",
      "Batchwise Average Test Loss: 7.715740e-01\n",
      "\n",
      "RMSE_V                   : 2.432713e-04\n",
      "\n",
      "RMSE_A                   : 1.154641e+01\n",
      "\n",
      "MAE_V                    : 1.957112e-04\n",
      "\n",
      "MAE_A                    : 9.724236e+00\n",
      "\n",
      "MaxAE_V                  : 1.195192e-03\n",
      "\n",
      "MaxAE_A                  : 3.117006e+01\n",
      "\n",
      "NRMSE_V                  : 2.214682e-05\n",
      "\n",
      "NRMSE_A                  : -2.156415e-01\n",
      "\n",
      "\n",
      "==================================================\n",
      "LG MVO Results:\n",
      "==================================================\n",
      "Batchwise Average Test Loss: 8.398986e-01\n",
      "\n",
      "RMSE_V                   : 9.570257e-04\n",
      "\n",
      "RMSE_A                   : 9.611234e+00\n",
      "\n",
      "MAE_V                    : 7.397885e-04\n",
      "\n",
      "MAE_A                    : 7.428412e+00\n",
      "\n",
      "MaxAE_V                  : 5.726933e-03\n",
      "\n",
      "MaxAE_A                  : 3.305994e+01\n",
      "\n",
      "NRMSE_V                  : 8.712526e-05\n",
      "\n",
      "NRMSE_A                  : -1.795001e-01\n",
      "\n",
      "\n",
      "==================================================\n",
      "NEGAT MVO Results:\n",
      "==================================================\n",
      "Batchwise Average Test Loss: 9.202956e-01\n",
      "\n",
      "RMSE_V                   : 1.081925e-03\n",
      "\n",
      "RMSE_A                   : 9.628668e+00\n",
      "\n",
      "MAE_V                    : 8.646236e-04\n",
      "\n",
      "MAE_A                    : 7.520331e+00\n",
      "\n",
      "MaxAE_V                  : 6.054044e-03\n",
      "\n",
      "MaxAE_A                  : 3.169131e+01\n",
      "\n",
      "NRMSE_V                  : 9.849580e-05\n",
      "\n",
      "NRMSE_A                  : -1.798257e-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_results(name, results):\n",
    "    \"\"\"Simple function to print results nicely\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name} Results:\")\n",
    "    print('='*50)\n",
    "    \n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric:25}: {value}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "print_results(\"FCNN MVO\", results_fcnn_MVO)\n",
    "print_results(\"GCNN+GAT MVO\", results_ngat_MVO)  \n",
    "print_results(\"GAT MVO\", results_gat_MVO)        \n",
    "print_results(\"LG MVO\", results_lg_MVO)          \n",
    "print_results(\"NEGAT MVO\", results_MVO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201620a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b5813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
