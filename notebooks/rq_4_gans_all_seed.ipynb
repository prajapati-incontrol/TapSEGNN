{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33c8f92",
   "metadata": {},
   "source": [
    "# GANs Implementation for Imputing Missing Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96393159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Indices for MVO are available from [0,141] \n",
      "\n",
      "Number of Trafos = 143 \n",
      " \n",
      "Network: MVO is selected \n",
      "\n",
      "Net MVO has 320 nodes and 318 edges. \n",
      "\n",
      "Filling nan as 0 in tap_pos, tap_neutral, tap_step_degree\n",
      "Scaling inputs...\n",
      "Number of V, P measurements 322 out of 640\n",
      "\n",
      "Number of P_to, Q_to, P_from, Q_from measurements 1770 out of 1944\n",
      "\n",
      "Sparsity of PV measurements at Buses = 50.0%\n",
      "Sparsity of P+ measurements at Lines = 50.0%\n",
      "Dataset for NEGATGenerator selected!\n",
      "\n",
      "\n",
      " Directed power flows accounted in dataset...\n",
      "\n",
      "\n",
      " get_edge_index_lu handling dictionary of tensors...\n",
      "\n",
      "Scaling inputs...\n",
      "Total number of parameters of model NEGATGenerator(\n",
      "  (node_layers): ModuleList(\n",
      "    (0): TAGConv(2, 64, K=3)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (fc_node): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (edge_layers): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0-1): 2 x TAGConv(6, 128, K=1)\n",
      "    )\n",
      "  )\n",
      "  (edge_biases): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])\n",
      "  (fc_edge): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (gatconv): GATConv(32, 32, heads=1)\n",
      "  (mlp_gat): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      "): 7363\n",
      "Total number of parameters of model DiffPoolDiscriminator(): 12145\n",
      "At epoch: 0, training: 8.506e+02, 0.49, 2.892e+02,                validation: 3.668e+02, 0.44, 0.000e+00, lr: 1.0e-05, 1.0e-02\n",
      "At epoch: 2, training: 1.717e+02, 0.19, 3.979e-01,                validation: 8.542e+01, 0.09, 5.922e-02, lr: 1.0e-05, 1.0e-02\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "import joblib \n",
    "import torch \n",
    "from typing import Literal, Tuple\n",
    "import seaborn as sns \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn \n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse, to_dense_batch\n",
    "from torch_geometric.nn.dense import dense_diff_pool\n",
    "from math import ceil\n",
    "from torch.optim import Optimizer\n",
    "from torch_geometric.loader import DataLoader\n",
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "from scipy import stats\n",
    "from scipy import spatial\n",
    "from scipy import special \n",
    "from matplotlib.patches import Rectangle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# to access the models and utils \n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from src.dataset.custom_dataset import DiscDataset, GenDataset\n",
    "from utils.model_utils import save_model\n",
    "from utils.ppnet_utils import initialize_network \n",
    "from utils.load_data_utils import load_sampled_input_data \n",
    "from utils.gen_utils import dataset_splitter, load_config, get_device,scale_numeric_columns, inverse_scale\n",
    "from src.model.graph_model import DiffPoolDiscriminator, NEGATGenerator\n",
    "\n",
    "seeds = [0]\n",
    "\n",
    "all_validation_d_losses = pd.DataFrame(columns=seeds)\n",
    "\n",
    "all_validation_d_accuracies = pd.DataFrame(columns=seeds)\n",
    "\n",
    "all_validation_g_losses = pd.DataFrame(columns=seeds)\n",
    "\n",
    "all_generated_p_values = pd.DataFrame(columns=seeds)\n",
    "all_real_p_values = pd.DataFrame(columns=seeds)\n",
    "\n",
    "all_generated_v_values = pd.DataFrame(columns=seeds)\n",
    "all_real_v_values = pd.DataFrame(columns=seeds)\n",
    "\n",
    "all_generated_pedge_values = pd.DataFrame(columns=seeds)\n",
    "all_real_pedge_values = pd.DataFrame(columns=seeds)\n",
    "\n",
    "for seed in seeds: \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    config = load_config('config_gan.yaml')\n",
    "\n",
    "    device = get_device(config['device'])\n",
    "\n",
    "    net = initialize_network(config['data']['net_name'], else_load=config['data']['load_std'])\n",
    "\n",
    "    # # make datasets and loader for Generator \n",
    "    sampled_input_data_G = load_sampled_input_data(sc_type=config['data']['gen_scenario_type'], \n",
    "                                                   net=net, \n",
    "                                                   num_samples=config['data']['num_samples'], \n",
    "                                                   noise=config['data']['noise'])\n",
    "\n",
    "    # # node input feature sparsity \n",
    "    nif_sparsity = np.count_nonzero(sampled_input_data_G['node_mask']) / sampled_input_data_G['node_mask'].numpy().size * 100\n",
    "    eif_sparsity = np.count_nonzero(sampled_input_data_G['edge_mask'][:,:,0]) / sampled_input_data_G['edge_mask'][:,:,0].numpy().size * 100\n",
    "\n",
    "    print(f\"Sparsity of PV measurements at Buses = {nif_sparsity:.1f}%\")\n",
    "    print(f\"Sparsity of P+ measurements at Lines = {eif_sparsity:.1f}%\")\n",
    "\n",
    "    dataset_G = GenDataset(model_name=config['model_G']['name'], sampled_input_data=sampled_input_data_G)\n",
    "\n",
    "    (train_loader_G, val_loader_G, test_loader_G), _ = dataset_splitter(dataset_G,\n",
    "                                                                batch_size=config['loader']['batch_size'])\n",
    "\n",
    "\n",
    "    # make datasets and loader for Discriminator\n",
    "    sampled_input_data_D = load_sampled_input_data(sc_type=config['data']['dis_scenario_type'], \n",
    "                                                net=net, \n",
    "                                                num_samples=config['data']['num_samples'],\n",
    "                                                p_std=config['data']['load_std'],\n",
    "                                                noise=config['data']['noise'],\n",
    "                                                p_true=1, # all are power flow results \n",
    "                                                )\n",
    "\n",
    " \n",
    "    dataset_D = DiscDataset(sampled_input_data=sampled_input_data_D)\n",
    "\n",
    "    (train_loader_D, val_loader_D, test_loader_D), _ = dataset_splitter(dataset=dataset_D, \n",
    "                                                                        batch_size=config['loader']['batch_size'])\n",
    "\n",
    "\n",
    "    # instantiate model, optimizer and schedular for Generator \n",
    "    model_G = NEGATGenerator(node_input_features=dataset_G[0][0].x.shape[-1], \n",
    "                        list_node_hidden_features=config['model_G']['list_node_hidden_features'], # [128,64], \n",
    "                        node_out_features=config['model_G']['node_out_features'], # 64, \n",
    "                        k_hop_node=config['model_G']['k_hop_node'], #1, \n",
    "                        edge_input_features=dataset_G[0][1].x.shape[-1],\n",
    "                        list_edge_hidden_features=config['model_G']['list_edge_hidden_features'], #[128,64], \n",
    "                        edge_output_features=config['model_G']['edge_out_features'], #64, \n",
    "                        k_hop_edge=config['model_G']['k_hop_edge'], #1, \n",
    "                        gat_out_features=config['model_G']['gat_out_features'], #32, \n",
    "                        gat_head=config['model_G']['gat_head'], #2, \n",
    "                        device=device)\n",
    "\n",
    "    optimizer_G = torch.optim.Adam(model_G.parameters(), \n",
    "                                lr=config['training_G']['lr'], \n",
    "                                weight_decay=config['training_G']['weight_decay'])\n",
    "\n",
    "    schedular_G = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_G, \n",
    "                                                        mode='min', \n",
    "                                                        factor=0.1, \n",
    "                                                        min_lr=config['training_G']['schedular_min_lr'])\n",
    "\n",
    "    total_params_G = sum(p.numel() for p in model_G.parameters() if p.requires_grad)\n",
    "    print(f'Total number of parameters of model {model_G}: {total_params_G}')\n",
    "\n",
    "\n",
    "    # instantiate model, optimizer and schedular for Discriminator \n",
    "    model_D = DiffPoolDiscriminator(in_channel=dataset_D[0].x.shape[-1], \n",
    "                                hidden_channel=config['model_D']['hidden_channel'], \n",
    "                                out_channel=config['model_D']['out_channel'], \n",
    "                                num_nodes=len(net.bus.index))\n",
    "\n",
    "    total_params_D = sum(p.numel() for p in model_D.parameters() if p.requires_grad)\n",
    "    print(f'Total number of parameters of model {model_D}: {total_params_D}')\n",
    "\n",
    "    optimizer_D = torch.optim.Adam(model_D.parameters(), \n",
    "                                lr=config['training_D']['lr'], \n",
    "                                weight_decay=config['training_D']['weight_decay'])\n",
    "\n",
    "    schedular_D = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_D, \n",
    "                                                        mode='min', \n",
    "                                                        factor=0.1, \n",
    "                                                        min_lr=config['training_D']['schedular_min_lr'])\n",
    "\n",
    "    def train_epoch_gan(model_G: nn.Module, \n",
    "                        model_D: nn.Module, \n",
    "                        loader_G: DataLoader, \n",
    "                        loader_D: DataLoader, \n",
    "                        optimizer_G: Optimizer, \n",
    "                        optimizer_D: Optimizer, \n",
    "                        disc_iter: int,\n",
    "                        gen_iter: int,\n",
    "                        device: Literal['cpu','cuda','mps'] = 'cpu') -> Tuple: \n",
    "        model_G.train()\n",
    "        model_D.train()\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # BCE Loss \n",
    "        dis_loss = 0\n",
    "        gen_loss = 0\n",
    "\n",
    "        # accuracy of discriminator  \n",
    "        correct = 0 \n",
    "        total = 0\n",
    "        \n",
    "\n",
    "        for fake_batch, real_batch in zip(loader_G, loader_D): \n",
    "            # Train discriminator \n",
    "            for _ in range(disc_iter): \n",
    "                optimizer_D.zero_grad()\n",
    "                # discriminator output logit for real samplesa as inputs\n",
    "                D_real_output = model_D(real_batch.x, real_batch.edge_index, real_batch.edge_attr, real_batch.batch)\n",
    "                \n",
    "                # loss of identifying real samples \n",
    "                real_loss = criterion(D_real_output, real_batch.y.to(device))\n",
    "\n",
    "                # accuracy of identifying real samples as real\n",
    "                D_real_output_binary = (torch.sigmoid(D_real_output) > 0.5).float()\n",
    "                correct += (D_real_output_binary == real_batch.y.to(device)).sum().item()\n",
    "                total += real_batch.y.shape[0]\n",
    "                \n",
    "\n",
    "                # discriminator output logit for fake samples as inputs \n",
    "                D_fake_output = model_D(model_G(fake_batch)[0], fake_batch[0].edge_index, model_G(fake_batch)[1], fake_batch[0].batch)\n",
    "\n",
    "                # loss of identifying fake samples \n",
    "                fake_loss = criterion(D_fake_output, fake_batch[0].y.to(device))\n",
    "\n",
    "                # accuracy of identifying fake samples as fake \n",
    "                D_fake_output_binary = (torch.sigmoid(D_fake_output) > 0.5).float()\n",
    "                correct += (D_fake_output_binary == fake_batch[0].y.to(device)).sum().item()\n",
    "                total += fake_batch[0].y.shape[0]\n",
    "\n",
    "                dis_net_loss = real_loss + fake_loss \n",
    "                dis_net_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                dis_loss += dis_net_loss.item()\n",
    "            \n",
    "            for _ in range(gen_iter):\n",
    "                # Train the generator to fool the discriminator \n",
    "                optimizer_G.zero_grad()\n",
    "                G_D_fake_output = model_D(model_G(fake_batch)[0], fake_batch[0].edge_index, model_G(fake_batch)[1], fake_batch[0].batch)\n",
    "                gen_train_loss = criterion(G_D_fake_output, fake_batch[0].y_fool.to(device))\n",
    "\n",
    "                gen_train_loss.backward()\n",
    "                optimizer_G.step()\n",
    "                gen_loss += gen_train_loss.item()\n",
    "        \n",
    "        disc_accuracy = correct / total \n",
    "        return dis_loss/(len(loader_D) + len(loader_G)), disc_accuracy, gen_loss/len(loader_G)\n",
    "\n",
    "    def val_epoch_gan(model_G: nn.Module, \n",
    "                        model_D: nn.Module, \n",
    "                        loader_G: DataLoader, \n",
    "                        loader_D: DataLoader, \n",
    "                        device: Literal['cpu','cuda','mps'] = 'cpu') -> Tuple: \n",
    "\n",
    "        model_G.eval()\n",
    "        model_D.eval()\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # BCE Loss \n",
    "        dis_loss = 0\n",
    "        gen_loss = 0\n",
    "\n",
    "        # accuracy of discriminator  \n",
    "        correct = 0 \n",
    "        total = 0\n",
    "\n",
    "\n",
    "        for fake_batch, real_batch in zip(loader_G, loader_D): \n",
    "            # Train discriminator \n",
    "            \n",
    "            # discriminator output logit for real samplesa as inputs\n",
    "            D_real_output = model_D(real_batch.x, real_batch.edge_index, real_batch.edge_attr, real_batch.batch)\n",
    "                \n",
    "            # loss of identifying real samples \n",
    "            real_loss = criterion(D_real_output, real_batch.y.to(device))\n",
    "\n",
    "            # accuracy of identifying real samples as real\n",
    "            D_real_output_binary = (torch.sigmoid(D_real_output) > 0.5).float()\n",
    "            correct += (D_real_output_binary == real_batch.y.to(device)).sum().item()\n",
    "            total += real_batch.y.shape[0]\n",
    "                \n",
    "\n",
    "            # discriminator output logit for fake samples as inputs \n",
    "            D_fake_output = model_D(model_G(fake_batch)[0], fake_batch[0].edge_index, model_G(fake_batch)[1], fake_batch[0].batch)\n",
    "\n",
    "            # loss of identifying fake samples \n",
    "            fake_loss = criterion(D_fake_output, fake_batch[0].y.to(device))\n",
    "\n",
    "            # accuracy of identifying fake samples as fake \n",
    "            D_fake_output_binary = (torch.sigmoid(D_fake_output) > 0.5).float()\n",
    "            correct += (D_fake_output_binary == fake_batch[0].y.to(device)).sum().item()\n",
    "            total += fake_batch[0].y.shape[0]\n",
    "\n",
    "            dis_net_loss = real_loss + fake_loss \n",
    "            dis_loss += dis_net_loss.item()\n",
    "            \n",
    "            # Train the generator to fool the discriminator \n",
    "            G_D_fake_output = model_D(model_G(fake_batch)[0], fake_batch[0].edge_index, model_G(fake_batch)[1], fake_batch[0].batch)\n",
    "            gen_train_loss = criterion(G_D_fake_output, fake_batch[0].y_fool.to(device))\n",
    "\n",
    "            gen_loss += gen_train_loss.item()\n",
    "        \n",
    "        disc_accuracy = correct / total\n",
    "\n",
    "        return dis_loss/(len(loader_D) + len(loader_G)), disc_accuracy, gen_loss/len(loader_G)\n",
    "\n",
    "    num_epoch = config['training_GAN']['num_epoch']\n",
    "    disc_iter = config['training_GAN']['disc_iter']\n",
    "    gen_iter = config['training_GAN']['gen_iter']\n",
    "\n",
    "    train_d_losses, train_d_accuracies, train_g_losses = [],[],[]\n",
    "    val_d_losses, val_d_accuracies, val_g_losses = [], [], []\n",
    "\n",
    "    start_tr = time.perf_counter()\n",
    "    early_stop = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        train_d_loss, train_d_accuracy, train_g_loss = train_epoch_gan(model_G = model_G, \n",
    "                                                                    model_D = model_D, \n",
    "                                                                    loader_G = train_loader_G, \n",
    "                                                                    loader_D = train_loader_D, \n",
    "                                                                    optimizer_G = optimizer_G, \n",
    "                                                                    optimizer_D = optimizer_D, \n",
    "                                                                    disc_iter = disc_iter,\n",
    "                                                                    gen_iter=gen_iter,\n",
    "                                                                    device = device)\n",
    "        \n",
    "        val_d_loss, val_d_accuracy, val_g_loss = val_epoch_gan(model_G = model_G, \n",
    "                                                            model_D = model_D, \n",
    "                                                            loader_G = val_loader_G, \n",
    "                                                            loader_D = val_loader_D, \n",
    "                                                            device = device)\n",
    "        train_d_losses.append(train_d_loss)\n",
    "        train_d_accuracies.append(train_d_accuracy)\n",
    "        train_g_losses.append(train_g_loss)\n",
    "\n",
    "        val_d_losses.append(val_d_loss)\n",
    "        val_d_accuracies.append(val_d_accuracy)\n",
    "        val_g_losses.append(val_g_loss)\n",
    "        \n",
    "        if schedular_G != None: \n",
    "            schedular_G.step(val_g_loss)\n",
    "        schedular_G_last_lr = schedular_G.get_last_lr()[0]\n",
    "        \n",
    "        if schedular_D != None: \n",
    "            schedular_D.step(val_d_loss)\n",
    "        schedular_D_last_lr = schedular_D.get_last_lr()[0]\n",
    "\n",
    "        if epoch % max(1, int(0.05 * num_epoch)) == 0: \n",
    "            print(f\"At epoch: {epoch}, training: {train_d_loss:.3e}, {train_d_accuracy:.2f}, {train_g_loss:.3e},\\\n",
    "                validation: {val_d_loss:.3e}, {val_d_accuracy:.2f}, {val_g_loss:.3e}, lr: {schedular_D_last_lr:.1e}, {schedular_G_last_lr:.1e}\")\n",
    "\n",
    "    end_tr = time.perf_counter()\n",
    "    elapsed_tr = end_tr - start_tr \n",
    "\n",
    "    all_validation_d_losses[seed] = val_d_losses\n",
    "    all_validation_d_accuracies[seed] = val_d_accuracies\n",
    "    all_validation_g_losses[seed] = val_g_losses\n",
    "\n",
    "    print(f\"GAN training took {elapsed_tr:.3f} seconds.\")\n",
    "\n",
    "    test_d_loss, test_d_accuracy, test_g_loss = val_epoch_gan(model_G = model_G, \n",
    "                                                            model_D = model_D, \n",
    "                                                            loader_G = test_loader_G, \n",
    "                                                            loader_D = test_loader_D, \n",
    "                                                            device = device)\n",
    "    print(f\"Test Performance: d_loss: {test_d_loss:.3e}, d_accuracy: {test_d_accuracy:.2f}, g_loss: {test_g_loss:.3e}\")\n",
    "\n",
    "\n",
    "    from datetime import datetime \n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_dir = f\"{parent_dir}/notebooks/rq4_gan_results_seed/{current_time}_{seed}/\"\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "    from matplotlib.patches import Rectangle\n",
    "\n",
    "    # Set publication-quality style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')  # Clean, professional look\n",
    "    sns.set_palette(\"husl\")  # Better color palette for academic figures\n",
    "\n",
    "    # Configure matplotlib for publication quality\n",
    "    fontsize=20\n",
    "    plt.rcParams.update({\n",
    "        'font.size': fontsize,\n",
    "        # 'font.family': 'serif',\n",
    "        # 'font.serif': ['Times New Roman', 'DejaVu Serif'],\n",
    "        'axes.labelsize': fontsize,\n",
    "        'axes.titlesize': fontsize,\n",
    "        'xtick.labelsize': 20,\n",
    "        'ytick.labelsize': 20,\n",
    "        'legend.fontsize': 20,\n",
    "        # 'figure.titlesize': ,\n",
    "        # 'text.usetex': True,  # Set to True if LaTeX is available\n",
    "        'axes.linewidth': 1.2,\n",
    "        'grid.alpha': 0.3,\n",
    "        'lines.linewidth': 2.5,\n",
    "        'lines.markersize': 6\n",
    "    })\n",
    "\n",
    "    # Create figure with proper DPI for publication\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8), dpi=300)\n",
    "\n",
    "    # Define professional color scheme\n",
    "    colors = {\n",
    "        'discriminator': '#1f77b4',  # Blue\n",
    "        'generator': '#ff7f0e',     # Orange\n",
    "        'accuracy': '#2ca02c'       # Green\n",
    "    }\n",
    "\n",
    "    # Plot losses with enhanced styling\n",
    "    epochs = range(len(train_d_losses))\n",
    "\n",
    "    line1 = ax1.plot(epochs, train_d_losses, \n",
    "                    color=colors['discriminator'], \n",
    "                    linewidth=2.5, \n",
    "                    label='Discriminator Loss',\n",
    "                    marker='o', \n",
    "                    markersize=4, \n",
    "                    markevery=max(1, len(epochs)//40),  # Show markers every 5% of epochs\n",
    "                    alpha=0.9)\n",
    "\n",
    "    line2 = ax1.plot(epochs, train_g_losses, \n",
    "                    color=colors['generator'], \n",
    "                    linewidth=2.5, \n",
    "                    label='Generator Loss',\n",
    "                    marker='s', \n",
    "                    markersize=4, \n",
    "                    markevery=max(1, len(epochs)//40),\n",
    "                    alpha=0.9)\n",
    "\n",
    "    # Create secondary y-axis for accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    line3 = ax2.plot(epochs, train_d_accuracies, \n",
    "                    color=colors['accuracy'], \n",
    "                    linewidth=2.5, \n",
    "                    label='Discriminator Accuracy',\n",
    "                    marker='^', \n",
    "                    markersize=4, \n",
    "                    markevery=max(1, len(epochs)//40),\n",
    "                    alpha=0.9)\n",
    "\n",
    "    # Enhanced axis formatting\n",
    "    ax1.set_xlabel('Training Epoch', fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontweight='bold', color='black')\n",
    "    ax2.set_ylabel('Accuracy', fontweight='bold', color=colors['accuracy'])\n",
    "\n",
    "    # Set title with better formatting\n",
    "    # fig.suptitle('GAN Training Dynamics: Loss and Accuracy Evolution', \n",
    "                #  fontsize=16, fontweight='bold', y=0.95)\n",
    "\n",
    "    # Improve grid appearance\n",
    "    ax1.grid(True, alpha=0.3, linestyle='-', linewidth=0.8)\n",
    "    ax2.grid(False)  # Avoid grid overlap\n",
    "\n",
    "    # Set axis limits with padding\n",
    "    ax1.set_xlim(-0.5, len(epochs)-0.5)\n",
    "    ax1.set_ylim(0, max(max(train_d_losses), max(train_g_losses)) * 1.1)\n",
    "    ax2.set_ylim(0, 1.05)  # Assuming accuracy is between 0 and 1\n",
    "\n",
    "    # # Format ticks\n",
    "    # ax1.xaxis.set_major_locator(MaxNLocator(integer=True, nbins=10))\n",
    "    # ax1.yaxis.set_major_locator(MaxNLocator(nbins=8))\n",
    "    # ax2.yaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "\n",
    "    # Color the right y-axis labels to match the accuracy line\n",
    "    ax2.tick_params(axis='y', labelcolor=colors['accuracy'])\n",
    "    ax2.spines['right'].set_color(colors['accuracy'])\n",
    "\n",
    "    # Create combined legend with better positioning\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, \n",
    "            loc='upper center', \n",
    "            #   bbox_to_anchor=(0.98, 0.3),\n",
    "            )\n",
    "\n",
    "    # Add convergence annotations (optional - customize based on your data)\n",
    "    # if len(train_d_losses) > 50:  # Only if sufficient training epochs\n",
    "    #     # Find approximate convergence points\n",
    "    #     convergence_epoch = len(train_d_losses) // 2  # Adjust based on your data\n",
    "        \n",
    "    #     # Add vertical line to highlight convergence region\n",
    "    #     ax1.axvline(x=convergence_epoch, color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    #     ax1.annotate('Approximate Convergence', \n",
    "    #                 xy=(convergence_epoch, max(train_g_losses)/2), \n",
    "    #                 xytext=(convergence_epoch + len(epochs)*0.1, max(train_g_losses)*0.8),\n",
    "    #                 arrowprops=dict(arrowstyle='->', color='gray', alpha=0.7),\n",
    "    #                 fontsize=10, color='gray')\n",
    "\n",
    "    # Improve layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)  # Make room for title\n",
    "\n",
    "    # Add subtle background shading to distinguish loss and accuracy regions\n",
    "    # (optional enhancement)\n",
    "    ax1.axhspan(ax1.get_ylim()[0], ax1.get_ylim()[1], \n",
    "            facecolor='lightblue', alpha=0.02, zorder=0)\n",
    "    # ax1.set_ylim([-1,10])\n",
    "\n",
    "    # Save figure in multiple formats for publication\n",
    "    plt.savefig(parent_dir + f'/notebooks/rq4_gan_results_seed/{current_time}_{seed}/FIG_ch_results_gan_training_dynamics.pdf', dpi=300, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.savefig(parent_dir + f'/notebooks/rq4_gan_results_seed/{current_time}_{seed}/FIG_ch_results_gan_training_dynamics.png', dpi=300, bbox_inches='tight',\n",
    "                facecolor='white', edgecolor='none')\n",
    "\n",
    "\n",
    "    # Optional: Create a separate convergence analysis plot\n",
    "    def create_convergence_plot():\n",
    "        \"\"\"Create a secondary plot focusing on convergence analysis\"\"\"\n",
    "        fig, ax2 = plt.subplots(1, 1, figsize=(12, 8), dpi=300, sharex=True)\n",
    "        \n",
    "        # # Top plot: Loss difference and moving averages\n",
    "        # window_size = max(1, len(train_d_losses) // 20)\n",
    "        # d_loss_ma = np.convolve(train_d_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "        # g_loss_ma = np.convolve(train_g_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "        \n",
    "        # ax1.plot(epochs, train_d_losses, alpha=0.3, color=colors['discriminator'])\n",
    "        # ax1.plot(epochs, train_g_losses, alpha=0.3, color=colors['generator'])\n",
    "        # ax1.plot(range(window_size-1, len(epochs)), d_loss_ma, \n",
    "        #          color=colors['discriminator'], linewidth=2, label='Discriminator Loss (MA)')\n",
    "        # ax1.plot(range(window_size-1, len(epochs)), g_loss_ma, \n",
    "        #          color=colors['generator'], linewidth=2, label='Generator Loss (MA)')\n",
    "        \n",
    "        # ax1.set_ylabel('Loss (Moving Average)', fontweight='bold')\n",
    "        # # ax1.set_title('Convergence Analysis: Moving Averages', fontweight='bold')\n",
    "        # ax1.legend()\n",
    "        # ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Bottom plot: Loss difference\n",
    "        loss_diff = np.array(train_d_losses) - np.array(train_g_losses)\n",
    "        ax2.plot(epochs, loss_diff, color='purple', linewidth=2, label='Loss Difference (D - G)')\n",
    "        ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='Nash Equilibrium')\n",
    "        ax2.fill_between(epochs, loss_diff, alpha=0.3, color='purple')\n",
    "        \n",
    "        ax2.set_xlabel('Training Epoch', fontweight='bold')\n",
    "        ax2.set_ylabel('Loss Difference', fontweight='bold')\n",
    "        # ax2.set_ylim([-1,10])\n",
    "        # ax2.set_title('Nash Equilibrium Analysis', fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(parent_dir + f'/notebooks/rq4_gan_results_seed/{current_time}_{seed}/FIG_ch_results_gan_convergence_analysis.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "    # Uncomment to create convergence analysis plot\n",
    "    create_convergence_plot()\n",
    "\n",
    "    try: \n",
    "        from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "        batch_G = next(iter(test_loader_G))\n",
    "        scaled_real_pv = batch_G[0].x_pfr\n",
    "        scaled_real_p_edge = batch_G[1].x_pfr[:,0]\n",
    "        scaled_gen_pv, scaled_gen_p_edge = model_G(batch_G)\n",
    "\n",
    "        real_pv = inverse_scale(scaled_real_pv, scaler=sampled_input_data_G['scaler_node'])\n",
    "        all_real_p_values[seed] = real_pv[:,0].detach().cpu().numpy()\n",
    "        all_real_v_values[seed] = real_pv[:,1].detach().cpu().numpy()\n",
    "\n",
    "        gen_pv = inverse_scale(scaled_gen_pv, scaler=sampled_input_data_G['scaler_node'])\n",
    "        all_generated_p_values[seed] = gen_pv[:,0].detach().cpu().numpy()\n",
    "        all_generated_v_values[seed] = gen_pv[:,1].detach().cpu().numpy()\n",
    "\n",
    "        p_edge_mean = sampled_input_data_G['scaler_edge'].mean_[0]\n",
    "        p_edge_var = sampled_input_data_G['scaler_edge'].var_[0]\n",
    "\n",
    "        real_p_edge = scaled_real_p_edge * p_edge_var + p_edge_mean\n",
    "        all_real_pedge_values[seed] = real_p_edge.detach().cpu().numpy()\n",
    "        \n",
    "        gen_p_edge = scaled_gen_p_edge * p_edge_var + p_edge_mean\n",
    "        all_generated_pedge_values[seed] = gen_p_edge.detach().cpu().numpy()\n",
    "\n",
    "        from scipy.stats import gaussian_kde\n",
    "        from scipy.special import rel_entr\n",
    "        from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "        # Convert tensors to numpy for plotting\n",
    "        real_v_np = real_pv[:,0].detach().cpu().numpy()\n",
    "        real_p_np = real_pv[:,1].detach().cpu().numpy()\n",
    "        generated_v_np = gen_pv[:,0].detach().cpu().numpy()\n",
    "        generated_p_np = gen_pv[:,1].detach().cpu().numpy()\n",
    "        real_p_edge_np = real_p_edge.detach().cpu().numpy()\n",
    "        gen_p_edge_np = gen_p_edge.detach().cpu().numpy()\n",
    "\n",
    "        # Create labeled DataFrames with better naming\n",
    "        real_df = pd.DataFrame({\n",
    "            'Power': real_p_np,\n",
    "            'Voltage': real_v_np,\n",
    "            'Type': 'Real Data'\n",
    "        })\n",
    "\n",
    "        gen_df = pd.DataFrame({\n",
    "            'Power': generated_p_np,\n",
    "            'Voltage': generated_v_np,\n",
    "            'Type': 'Generated Data'\n",
    "        })\n",
    "\n",
    "        # Combine datasets\n",
    "        combined_data = pd.concat([real_df, gen_df], ignore_index=True)\n",
    "\n",
    "        # Set up the plot with enhanced styling\n",
    "        plt.style.use('default')  # Keep default matplotlib style\n",
    "        sns.set_palette([\"#2E86AB\", \"#A23B72\"])  # Professional blue and magenta\n",
    "\n",
    "        # Create the enhanced jointplot\n",
    "        g = sns.jointplot(\n",
    "            data=combined_data,\n",
    "            x='Power',\n",
    "            y='Voltage',\n",
    "            hue='Type',\n",
    "            kind='kde',\n",
    "            fill=True,\n",
    "            alpha=0.6,\n",
    "            height=10,\n",
    "            ratio=5,\n",
    "            space=0.1,\n",
    "            fontsize=fontsize,\n",
    "            marginal_kws={'alpha': 0.7, 'linewidth': 2},\n",
    "            joint_kws={'alpha': 0.6, 'linewidths': 1.5},\n",
    "            legend=True,  \n",
    "        )\n",
    "\n",
    "        # # Enhance the main plot\n",
    "        g.ax_joint.grid(True, alpha=0.3, linestyle='-', linewidth=0.8)\n",
    "\n",
    "\n",
    "        # Enhance marginal plots\n",
    "        g.ax_marg_x.grid(True, alpha=0.3)\n",
    "        g.ax_marg_y.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add statistical annotations\n",
    "        real_power_mean = real_p_np.mean()\n",
    "        real_voltage_mean = real_v_np.mean()\n",
    "        gen_power_mean = generated_p_np.mean()\n",
    "        gen_voltage_mean = generated_v_np.mean()\n",
    "\n",
    "        # Add mean markers\n",
    "        g.ax_joint.scatter(real_power_mean, real_voltage_mean, \n",
    "                        marker='x', s=200, color='#2E86AB', \n",
    "                        linewidth=3, label='Real Mean', zorder=5)\n",
    "        g.ax_joint.scatter(gen_power_mean, gen_voltage_mean, \n",
    "                        marker='x', s=200, color='#A23B72', \n",
    "                        linewidth=3, label='Generated Mean', zorder=5)\n",
    "\n",
    "        # Calculate and display statistical metrics\n",
    "        def calculate_metrics(real_data, gen_data):\n",
    "            \"\"\"Calculate JS and KL divergence for Power and Voltage columns.\"\"\"\n",
    "            \n",
    "            def compute_divergences(real_series, gen_series):\n",
    "                # Combine data for common evaluation range\n",
    "                data = np.concatenate([real_series, gen_series])\n",
    "                x = np.linspace(data.min(), data.max(), 1000)\n",
    "                \n",
    "                # KDE estimation\n",
    "                real_kde = gaussian_kde(real_series)\n",
    "                gen_kde = gaussian_kde(gen_series)\n",
    "\n",
    "                # Evaluate densities\n",
    "                real_prob = real_kde(x)\n",
    "                gen_prob = gen_kde(x)\n",
    "\n",
    "                # Add epsilon to avoid zeros\n",
    "                epsilon = 1e-10\n",
    "                real_prob += epsilon\n",
    "                gen_prob += epsilon\n",
    "\n",
    "                # Re-normalize after epsilon adjustment\n",
    "                real_prob /= np.sum(real_prob)\n",
    "                gen_prob /= np.sum(gen_prob)\n",
    "\n",
    "                # Compute divergences\n",
    "                kl = np.sum(rel_entr(real_prob, gen_prob))  # KL(real || gen)\n",
    "                js = jensenshannon(real_prob, gen_prob, base=2) ** 2  # Square to get divergence\n",
    "\n",
    "                return js, kl\n",
    "\n",
    "            # Compute metrics for both Power and Voltage\n",
    "            js_div_power, kl_div_power = compute_divergences(real_data['Power'], gen_data['Power'])\n",
    "            js_div_voltage, kl_div_voltage = compute_divergences(real_data['Voltage'], gen_data['Voltage'])\n",
    "\n",
    "            return js_div_power, js_div_voltage, kl_div_power, kl_div_voltage\n",
    "\n",
    "        js_div_power, js_div_voltage, kl_div_power, kl_div_voltage = calculate_metrics(real_df, gen_df)\n",
    "\n",
    "        # Add text box with statistical information\n",
    "        stats_text = f\"\"\"JS Divergence:\n",
    "        Power: {js_div_power:.4f}\n",
    "        Voltage: {js_div_voltage:.4f}\n",
    "\n",
    "        KL Divergence:\n",
    "        Power: {kl_div_power:.4f}\n",
    "        Voltage: {kl_div_voltage:.4f}\n",
    "\n",
    "        Sample Size:\n",
    "        Real: {len(real_df)}\n",
    "        Generated: {len(gen_df)}\"\"\"\n",
    "\n",
    "        g.ax_joint.text(0.02, 0.98, stats_text,\n",
    "                    transform=g.ax_joint.transAxes,\n",
    "                    fontsize=16,\n",
    "                    verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', \n",
    "                                facecolor='white', \n",
    "                                alpha=0.8,\n",
    "                                edgecolor='gray'))\n",
    "\n",
    "        # Get existing legend handles and labels from the seaborn plot\n",
    "        handles, labels = g.ax_joint.get_legend_handles_labels()\n",
    "\n",
    "        # Create new handles \n",
    "        from matplotlib.lines import Line2D\n",
    "        mean_handles = [\n",
    "            Line2D([0], [0], marker='o', color='#2E86AB', linewidth=0, markersize=10, markeredgewidth=3),\n",
    "            Line2D([0], [0], marker='o', color='#A23B72', linewidth=0, markersize=10, markeredgewidth=3)\n",
    "        ]\n",
    "\n",
    "        # Combine all handles and labels\n",
    "        all_handles = handles + mean_handles\n",
    "        all_labels = labels + ['Real Distribution', 'Generated Distribution']\n",
    "\n",
    "        # Create the complete legend\n",
    "        g.ax_joint.legend(handles=all_handles, \n",
    "                        labels=all_labels,\n",
    "                        loc='lower left', \n",
    "                        fontsize=16)\n",
    "\n",
    "        # Enhance legend with increased font size\n",
    "        # g.ax_joint.legend(loc='lower right', \n",
    "        #                  fontsize=14,  # Use your fontsize variable\n",
    "        #                 #  frameon=True, \n",
    "        #                 #  fancybox=True, \n",
    "        #                 #  shadow=True,\n",
    "        #                 #  framealpha=0.9\n",
    "        #                  )\n",
    "\n",
    "        g.ax_joint.set_xlabel(\"$P$\",fontsize=fontsize)\n",
    "        g.ax_joint.set_ylabel(\"$|V\\u0332|$\",fontsize=fontsize)\n",
    "        g.ax_joint.tick_params(axis='x', labelsize=fontsize)\n",
    "        g.ax_joint.tick_params(axis='y', labelsize=fontsize)\n",
    "\n",
    "        # Improve layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save in multiple formats\n",
    "        plt.savefig(parent_dir + f'/notebooks/rq4_gan_results_seed/{current_time}_{seed}/FIG_ch_results_GAN_power_voltage_kde_comparison.pdf', dpi=300, bbox_inches='tight',\n",
    "                    facecolor='white', edgecolor='none')\n",
    "        plt.savefig(parent_dir + f'/notebooks/rq4_gan_results_seed/{current_time}_{seed}/FIG_ch_results_GAN_power_voltage_kde_comparison.png', dpi=300, bbox_inches='tight',\n",
    "                    facecolor='white', edgecolor='none')\n",
    "\n",
    "        from scipy.stats import gaussian_kde, entropy\n",
    "        from scipy.spatial.distance import jensenshannon\n",
    "        from scipy.special import rel_entr\n",
    "        from scipy import stats\n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "        # Calculate and display statistical metrics\n",
    "        def calculate_metrics(real_data, gen_data):\n",
    "            \"\"\"Calculate JS and KL divergence for Power and Voltage columns.\"\"\"\n",
    "            def compute_divergences(real_series, gen_series):\n",
    "                # Combine data for common evaluation range\n",
    "                data = np.concatenate([real_series, gen_series])\n",
    "                x = np.linspace(data.min(), data.max(), 1000)\n",
    "                \n",
    "                # KDE estimation\n",
    "                real_kde = gaussian_kde(real_series)\n",
    "                gen_kde = gaussian_kde(gen_series)\n",
    "                \n",
    "                # Evaluate densities\n",
    "                real_prob = real_kde(x)\n",
    "                gen_prob = gen_kde(x)\n",
    "                \n",
    "                # Add epsilon to avoid zeros\n",
    "                epsilon = 1e-10\n",
    "                real_prob += epsilon\n",
    "                gen_prob += epsilon\n",
    "                \n",
    "                # Re-normalize after epsilon adjustment\n",
    "                real_prob /= np.sum(real_prob)\n",
    "                gen_prob /= np.sum(gen_prob)\n",
    "                \n",
    "                # Compute divergences\n",
    "                kl = np.sum(rel_entr(real_prob, gen_prob))  # KL(real || gen)\n",
    "                js = jensenshannon(real_prob, gen_prob, base=2) ** 2  # Square to get divergence\n",
    "                \n",
    "                return js, kl\n",
    "            \n",
    "            # For edge power, we need to handle single series\n",
    "            if isinstance(real_data, (pd.Series, np.ndarray)) and isinstance(gen_data, (pd.Series, np.ndarray)):\n",
    "                js_div, kl_div = compute_divergences(real_data, gen_data)\n",
    "                return js_div, kl_div\n",
    "            else:\n",
    "                # Compute metrics for both Power and Voltage\n",
    "                js_div_power, kl_div_power = compute_divergences(real_data['Power'], gen_data['Power'])\n",
    "                js_div_voltage, kl_div_voltage = compute_divergences(real_data['Voltage'], gen_data['Voltage'])\n",
    "                return js_div_power, js_div_voltage, kl_div_power, kl_div_voltage\n",
    "\n",
    "        # Create a DataFrame for plotting\n",
    "        df_edge = pd.DataFrame({\n",
    "            'Value': np.concatenate([real_p_edge_np, gen_p_edge_np]),\n",
    "            'Type': ['Real Data'] * len(real_p_edge_np) + ['Generated Data'] * len(gen_p_edge_np)\n",
    "        })\n",
    "\n",
    "        # Calculate divergence metrics\n",
    "        js_divergence, kl_divergence = calculate_metrics(real_p_edge_np, gen_p_edge_np)\n",
    "\n",
    "        # Set up enhanced plotting style\n",
    "        plt.style.use('default')\n",
    "        fig, ax = plt.subplots(figsize=(12, 8), dpi=300)\n",
    "\n",
    "        # Enhanced color palette\n",
    "        colors = ['#2E86AB', '#A23B72']  # Professional blue and magenta\n",
    "        sns.set_palette(colors)\n",
    "\n",
    "        # Create the enhanced KDE plot\n",
    "        kde_plot = sns.kdeplot(\n",
    "            data=df_edge,\n",
    "            x='Value',\n",
    "            hue='Type',\n",
    "            fill=True,\n",
    "            common_norm=False,\n",
    "            alpha=0.6,\n",
    "            linewidth=2.5,\n",
    "            ax=ax,\n",
    "            # legend=True,\n",
    "        )\n",
    "\n",
    "        # kde_plot.legend()\n",
    "\n",
    "        # Add unfilled KDE lines for better definition\n",
    "        sns.kdeplot(\n",
    "            data=df_edge,\n",
    "            x='Value',\n",
    "            hue='Type',\n",
    "            fill=True,\n",
    "            common_norm=False,\n",
    "            linewidth=3,\n",
    "            ax=ax,\n",
    "            # legend=True,\n",
    "        )\n",
    "\n",
    "        # Enhanced axis labels and title\n",
    "        ax.set_xlabel('$P^+$', fontsize=fontsize)\n",
    "        ax.set_ylabel('KDE Density', fontsize=fontsize)\n",
    "        ax.tick_params(axis='x', labelsize=fontsize)\n",
    "        ax.tick_params(axis='y', labelsize=fontsize)\n",
    "\n",
    "        # ax.set_title('Distribution Comparison: Real vs Generated Edge Power',fontsize=fontsize, pad=20)\n",
    "\n",
    "        # Add grid\n",
    "        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.8)\n",
    "\n",
    "        # Create comprehensive statistics text box\n",
    "        stats_text = f\"\"\"Divergence Measures:\n",
    "        JS Divergence: {js_divergence:.6f}\n",
    "        KL Divergence: {kl_divergence:.6f}\n",
    "        \n",
    "        Sample Sizes:\n",
    "        Real: {len(real_p_edge_np)}, Generated: {len(gen_p_edge_np)}\"\"\"\n",
    "\n",
    "        # Add statistics text box\n",
    "        ax.text(0.58, 0.98, stats_text,\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=fontsize,\n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round,pad=0.8', \n",
    "                        facecolor='white', \n",
    "                        alpha=0.9,\n",
    "                        edgecolor='gray',\n",
    "                        linewidth=1))\n",
    "\n",
    "        # Create new handles \n",
    "        from matplotlib.lines import Line2D\n",
    "        mean_handles = [\n",
    "            Line2D([0], [0], marker='o', color='#2E86AB', linewidth=0, markersize=10, markeredgewidth=3),\n",
    "            Line2D([0], [0], marker='o', color='#A23B72', linewidth=0, markersize=10, markeredgewidth=3)\n",
    "        ]\n",
    "\n",
    "        # Combine all handles and labels\n",
    "        all_handles = mean_handles\n",
    "        all_labels = ['Real Distribution', 'Generated Distribution']\n",
    "\n",
    "        # Create the complete legend\n",
    "        ax.legend(handles=all_handles, \n",
    "                        labels=all_labels,\n",
    "                        loc='center right', \n",
    "                        fontsize=fontsize)\n",
    "\n",
    "        # Enhance the plot area\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_linewidth(1.2)\n",
    "        ax.spines['bottom'].set_linewidth(1.2)\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save in multiple formats\n",
    "        plt.savefig(parent_dir + f'/notebooks/rq4_gan_results_seed/{current_time}_{seed}/FIG_ch_results_GAN_edge_power_kde_enhanced.pdf', dpi=300, bbox_inches='tight',\n",
    "                    facecolor='white', edgecolor='none')\n",
    "        plt.savefig(parent_dir + f'/notebooks/rq4_gan_results_seed/{current_time}_{seed}/FIG_ch_results_GAN_edge_power_kde_enhanced.png', dpi=300, bbox_inches='tight',\n",
    "                    facecolor='white', edgecolor='none')\n",
    "    except Exception as e: \n",
    "        print(f\"At seed {seed}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfaffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the DataFrames are already filled and indexed by epoch:\n",
    "# all_validation_d_losses, all_validation_d_accuracies, all_validation_g_losses\n",
    "\n",
    "def plot_with_variance(ax, df, title, ylabel, color):\n",
    "    mean_vals = df.mean(axis=1)\n",
    "    std_vals = df.std(axis=1)\n",
    "    epochs = df.index\n",
    "\n",
    "    ax.plot(epochs, mean_vals, label='Mean', color=color)\n",
    "    ax.fill_between(epochs, mean_vals - std_vals, mean_vals + std_vals,\n",
    "                    alpha=0.3, color=color, label='Std Dev')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "plot_with_variance(axes[0], all_validation_d_losses,\n",
    "                   \"Discriminator Validation Loss\", \"Loss\", color=\"tab:blue\")\n",
    "\n",
    "plot_with_variance(axes[1], all_validation_g_losses,\n",
    "                   \"Generator Validation Loss\", \"Loss\", color=\"tab:orange\")\n",
    "\n",
    "plot_with_variance(axes[2], all_validation_d_accuracies,\n",
    "                   \"Discriminator Validation Accuracy\", \"Accuracy\", color=\"tab:green\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_variance_sns(ax, df, title, ylabel, color, font_sizes=None):\n",
    "    if font_sizes is None:\n",
    "        font_sizes = {'title': 12, 'label': 10, 'tick': 9, 'legend': 9}\n",
    "    \n",
    "    mean_vals = df.mean(axis=1)\n",
    "    std_vals = df.std(axis=1)\n",
    "    epochs = df.index\n",
    "    \n",
    "    # Plot mean line\n",
    "    ax.plot(epochs, mean_vals, label='Mean', color=color, linewidth=2)\n",
    "    \n",
    "    # Fill between for standard deviation\n",
    "    ax.fill_between(epochs, np.maximum(mean_vals - std_vals, 0), mean_vals + std_vals,\n",
    "                    alpha=0.3, color=color, label='±1 Std Dev')\n",
    "    \n",
    "    # Set labels and title with custom font sizes\n",
    "    # ax.set_title(title, fontsize=font_sizes['title'], fontweight='bold')\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=font_sizes['label'])\n",
    "    ax.set_ylabel(ylabel, fontsize=font_sizes['label'])\n",
    "    \n",
    "    # Customize tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=font_sizes['tick'])\n",
    "    \n",
    "    # Set x-tick labels to integers only\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    \n",
    "    # Legend with custom font size\n",
    "    ax.legend(fontsize=font_sizes['legend'])\n",
    "    \n",
    "    # Apply seaborn styling\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "# Set seaborn style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # Clean, professional look\n",
    "sns.set_palette(\"husl\")  # Better color palette for academic figures\n",
    "\n",
    "# Define font sizes (easily tunable)\n",
    "font_config = {\n",
    "    # 'title': 14,  # Title font size\n",
    "    'label': 20,   # Axis label font size\n",
    "    'tick': 14,    # Tick label font size\n",
    "    'legend': 14   # Legend font size\n",
    "}\n",
    "\n",
    "# Apply seaborn context for overall scaling\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "# Plot discriminator validation loss\n",
    "# all_validation_d_losses_norm = all_validation_d_losses / all_validation_d_losses.max().max()\n",
    "plot_with_variance_sns(axes, all_validation_d_losses,\n",
    "                      \"Discriminator Validation Loss\", \"Loss\",\n",
    "                      color=sns.color_palette(\"husl\", 3)[0],\n",
    "                      font_sizes=font_config)\n",
    "axes.set_xlim([15,100]), axes.set_ylim([-100,100])\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig2, axes2 = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "# all_validation_g_losses_norm = all_validation_g_losses / all_validation_g_losses.max().max()\n",
    "plot_with_variance_sns(axes2, all_validation_g_losses,\n",
    "                      \"Generator Validation Loss\", \"Loss\",\n",
    "                      color=sns.color_palette(\"husl\", 3)[1],\n",
    "                      font_sizes=font_config)\n",
    "axes2.set_xlim([15,100]), axes2.set_ylim([-100,100])\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig3, axes3 = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "plot_with_variance_sns(axes3, all_validation_d_accuracies,\n",
    "                      \"Discriminator Validation Accuracy\", \"Accuracy\",\n",
    "                      color=sns.color_palette(\"husl\", 3)[2],\n",
    "                      font_sizes=font_config)\n",
    "axes3.set_ylim([0,1])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(all_generated_p_values, parent_dir + '/notebooks/rq4_gan_results_seed/all_generated_p_values.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e2d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(all_real_pedge_values, parent_dir + '/notebooks/rq4_gan_results_seed/all_real_pedge_values.pkl')\n",
    "joblib.dump(all_generated_v_values, parent_dir + '/notebooks/rq4_gan_results_seed/all_generated_v_values.pkl')\n",
    "joblib.dump(all_generated_pedge_values, parent_dir + '/notebooks/rq4_gan_results_seed/all_generated_pedge_values.pkl')\n",
    "joblib.dump(all_real_p_values, parent_dir + '/notebooks/rq4_gan_results_seed/all_real_p_values.pkl')\n",
    "joblib.dump(all_real_v_values, parent_dir + '/notebooks/rq4_gan_results_seed/all_real_v_values.pkl')\n",
    "\n",
    "\n",
    "joblib.dump(all_validation_d_losses, parent_dir + '/notebooks/rq4_gan_results_seed/all_validation_d_losses.pkl')\n",
    "joblib.dump(all_validation_g_losses, parent_dir + '/notebooks/rq4_gan_results_seed/all_validation_g_losses.pkl')\n",
    "joblib.dump(all_validation_d_accuracies, parent_dir + '/notebooks/rq4_gan_results_seed/all_validation_d_accuracies.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c46519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
